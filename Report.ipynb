{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report for Continuous Control Project\n",
    "\n",
    "## Environment description\n",
    "The output below is from the unity environment initialisation\n",
    "\n",
    "Unity brain name: ReacherBrain\n",
    "        Number of Visual Observations (per agent): 0\n",
    "        Vector Observation space type: continuous\n",
    "        Vector Observation space size (per agent): 33\n",
    "        Number of stacked Vector Observation: 1\n",
    "        Vector Action space type: continuous\n",
    "        Vector Action space size (per agent): 4\n",
    "        Vector Action descriptions: , , , \n",
    "Number of agents: 20\n",
    "Size of each action: 4\n",
    "There are 20 agents. Each observes a state with length: 33\n",
    "        \n",
    "This shows there are 33 State Spacea, and 4 Action spaces for the environemnt.\n",
    "\n",
    "### Solving the Environment\n",
    "\n",
    "The Environment is considered solved when an average score of +30 or more is achieved over 100 episodes.\n",
    "        \n",
    "## Model Description\n",
    "This project uses PyTorch to construct a neural network for each of the Actor and Critic portions of the agent:\n",
    "\n",
    "The Actor Has:\n",
    "    - 33 inputs (State size from environment) \n",
    "    - 512 First fully connected hidden layer \n",
    "    - 512 Second Fully connected hidden layer \n",
    "    - 4 output (Action size from environment)\n",
    "    \n",
    "The Critic has:\n",
    "    - 33 inputs (State size from environment) \n",
    "    - 512 First fully connected hidden layer \n",
    "    - 512 Second Fully connected hidden layer \n",
    "    - 1 output (Value for the (State, Action) pair)\n",
    "     \n",
    "The hidden layers all use the Relu activation function.\n",
    "\n",
    "## Agent Description\n",
    "The algorithm used for the solution is a DDPG agent, this agent uses 4 neural networks in order to learn the task. \n",
    "    - Actor Local\n",
    "    - Actor Target\n",
    "    - Critic Local\n",
    "    - Critic Target\n",
    "\n",
    "### Q-Network\n",
    "The goal of a Q Network is to learn the optimal policy to solve a given task. This is achieved by initially performing random action given the current state and using the reward from the environment to update the likelyhood of choosing that response again. Over time the Q-Network will determine the best action to perform given a state from the environment. When a policy that performes the best action for each state has been found then the optimal policy has been reached.\n",
    "\n",
    "### DQN\n",
    "A DQN is a reinforcement learning Algorithm that uses a neural network for the function approximator. In this instance the model described above is used at the centre of the algorithm to learn the required actions to perform for a given environment state.\n",
    "\n",
    "### DDPG\n",
    "The DDPG agent uses a similar methodology to the DQN agent. DQN agent become much more complecated to implement if the action space of an environment is continuous. The DDPG agent solves this issue by using 2 neural networks, one (the actor) determines the best action under the current policy, the other (the critic) determines the best action value for the action chosen by the actor. This allows a value to be assigned to an action.\n",
    "\n",
    "### Random Replay buffer\n",
    "The Replay Buffer is storage that contains the results of all actions taken. Instead of learning while taking every action the replay buffer provides a list of previously taken actions. At pre determined intervals a number of samples are chosen from the buffer at random this means that the training data is out of sequence. This removes the possibility of certain sequences biasing the training of the neural network. In this porject 10 training loops are performed every 20 timesteps.\n",
    "\n",
    "\n",
    "### Soft Target Update\n",
    "When using soft target updates the changes of the weights in the network are adapted in small steps usinghte equation below:\n",
    "\n",
    "![](results/softUpdate.png)\n",
    "\n",
    "Research has shown the soft update method has better results than a hard update method, where the steps taken are larger.\n",
    "\n",
    "### Noise\n",
    "In order to produce random varience in the choice of action a noise is introcuced on to the chosen action on each timestep. The noise in this project is generated by random normal distrobution function.\n",
    "\n",
    "\n",
    "## Hyperparameters\n",
    "The current Hyperparameter list is shown below.\n",
    "\n",
    "    -BUFFER_SIZE = int(1e5)  # replay buffer size\n",
    "    -BATCH_SIZE = 1024        # minibatch size\n",
    "    -GAMMA = 0.99            # discount factor\n",
    "    -TAU = 1e-3              # for soft update of target parameters\n",
    "    -LR_ACTOR = 1e-3         # learning rate of the actor \n",
    "    -LR_CRITIC = 1e-3        # learning rate of the critic\n",
    "    -WEIGHT_DECAY = 0.       # weight decay\n",
    "    \n",
    "# Results\n",
    "The DDPG Agent Developed in this project was able to reach a score of >30 in 40 episodes and keep the score above 30 for over 100 episodes.\n",
    "\n",
    "![](results/ScoreImage.png)\n",
    "\n",
    "\n",
    "# Future Development\n",
    "There are some known drwbacks with the implementstion in this project due to limitations in the application of the agent. The following processes could improve the performance of the training agent:\n",
    "\n",
    "## Replay Buffer\n",
    "One improvement could be to implement a Prioritised Experience Replay buffer. In the current imlementation all experiences in the replay buffer have equal probability of being chosen. With a priotitised buffer the experiences would be pritoritised in terms of how much the result diverges from the predicted results.\n",
    "\n",
    "\n",
    "## Other Agent Types\n",
    "It may be worth experimenting with other agents such as D4PG, TRPO  and TNPG implementations to investigate the best approach for solving the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
